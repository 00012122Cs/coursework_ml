{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# 03 - Model Training & Evaluation\nThis notebook trains multiple regressors to predict life expectancy, performs hyperparameter tuning, and compares model performance."
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Workflow\n1. Load the cleaned dataset from `data/processed/`.\n2. Build training and evaluation utilities (metrics, plotting helpers).\n3. Define preprocessing + model pipelines for Linear Regression, Random Forest, and Gradient Boosting.\n4. Tune models with `GridSearchCV`, evaluate on the hold-out set, and visualise predictions.\n5. Save the best-performing pipeline to `models/final_model.pkl` and log the comparison table."
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "from pathlib import Path\nfrom typing import Dict\n\nimport joblib\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport plotly.express as px\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\nfrom sklearn.impute import KNNImputer, SimpleImputer\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\nfrom sklearn.model_selection import GridSearchCV, train_test_split\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\n\nDATA_PATH = Path('../data/processed/life_expectancy_clean.csv')\nMODELS_DIR = Path('../models')\nMODELS_DIR.mkdir(parents=True, exist_ok=True)"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "def load_clean_data(path: Path = DATA_PATH) -> pd.DataFrame:\n    return pd.read_csv(path)\n\n\ndef build_preprocessor() -> ColumnTransformer:\n    numeric_features = [\n        'year', 'life_expectancy_low', 'life_expectancy_high', 'value_range',\n        'year_normalized', 'continent_life_expectancy_mean', 'country_life_expectancy_mean',\n        'continent_encoded'\n    ]\n    categorical_features = ['gender', 'continent', 'country_code']\n\n    numeric_transformer = Pipeline([\n        ('imputer', KNNImputer(n_neighbors=5)),\n        ('scaler', StandardScaler())\n    ])\n    categorical_transformer = Pipeline([\n        ('imputer', SimpleImputer(strategy='most_frequent')),\n        ('encoder', OneHotEncoder(handle_unknown='ignore', sparse=False))\n    ])\n\n    return ColumnTransformer([\n        ('numeric', numeric_transformer, numeric_features),\n        ('categorical', categorical_transformer, categorical_features)\n    ])\n\n\ndef regression_metrics(y_true, y_pred) -> Dict[str, float]:\n    mae = mean_absolute_error(y_true, y_pred)\n    rmse = mean_squared_error(y_true, y_pred, squared=False)\n    r2 = r2_score(y_true, y_pred)\n    return {'MAE': mae, 'RMSE': rmse, 'R2': r2}\n\n\ndef plot_predictions(y_true, y_pred, title: str) -> None:\n    plt.figure(figsize=(6, 6))\n    plt.scatter(y_true, y_pred, alpha=0.6)\n    lims = [min(y_true.min(), y_pred.min()), max(y_true.max(), y_pred.max())]\n    plt.plot(lims, lims, 'r--')\n    plt.xlabel('Actual')\n    plt.ylabel('Predicted')\n    plt.title(title)\n    plt.show()"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "df = load_clean_data()\nfeature_cols = [\n    'year', 'gender', 'continent', 'country_code', 'life_expectancy_low', 'life_expectancy_high',\n    'value_range', 'year_normalized', 'continent_life_expectancy_mean',\n    'country_life_expectancy_mean', 'continent_encoded'\n]\ntarget_col = 'life_expectancy'\n\nX = df[feature_cols]\ny = df[target_col]\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42, shuffle=True\n)\nX_train.shape, X_test.shape"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "models_config = {\n    'Linear Regression': {\n        'model': LinearRegression(),\n        'params': {\n            'regressor__fit_intercept': [True, False]\n        }\n    },\n    'Random Forest': {\n        'model': RandomForestRegressor(random_state=42),\n        'params': {\n            'regressor__n_estimators': [200, 300],\n            'regressor__max_depth': [None, 10, 20],\n            'regressor__min_samples_split': [2, 5]\n        }\n    },\n    'Gradient Boosting': {\n        'model': GradientBoostingRegressor(random_state=42),\n        'params': {\n            'regressor__n_estimators': [150, 200],\n            'regressor__learning_rate': [0.05, 0.1],\n            'regressor__max_depth': [2, 3]\n        }\n    }\n}"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "results = []\nbest_models = {}\n\nfor model_name, cfg in models_config.items():\n    print(f'\\nTraining {model_name}...')\n    pipeline = Pipeline([\n        ('preprocessor', build_preprocessor()),\n        ('regressor', cfg['model'])\n    ])\n\n    grid = GridSearchCV(\n        pipeline,\n        param_grid=cfg['params'],\n        cv=5,\n        scoring='neg_mean_absolute_error',\n        n_jobs=-1\n    )\n    grid.fit(X_train, y_train)\n    y_pred = grid.predict(X_test)\n    metrics = regression_metrics(y_test, y_pred)\n    metrics['Model'] = model_name\n    metrics['Best Params'] = grid.best_params_\n    results.append(metrics)\n    best_models[model_name] = grid.best_estimator_\n    plot_predictions(y_test, y_pred, f'{model_name} Predictions')\n\nresults_df = pd.DataFrame(results)\nresults_df"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "results_long = results_df.melt(id_vars=['Model', 'Best Params'], value_vars=['MAE', 'RMSE', 'R2'],\n                                 var_name='Metric', value_name='Value')\nfig = px.bar(results_long, x='Model', y='Value', color='Metric', barmode='group', title='Model Comparison')\nfig.show()"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "best_model_name = results_df.sort_values(by='R2', ascending=False).iloc[0]['Model']\nbest_pipeline = best_models[best_model_name]\nmodel_path = MODELS_DIR / 'final_model.pkl'\nmetrics_path = MODELS_DIR / 'model_performance.csv'\n\njoblib.dump(best_pipeline, model_path)\nresults_df.to_csv(metrics_path, index=False)\n\nprint(f'Saved best model ({best_model_name}) to {model_path}')\nresults_df[['Model', 'MAE', 'RMSE', 'R2']]"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Findings & Next Steps\n- The table summarises MAE, RMSE, and R\u00b2, enabling objective model selection.\n- The saved pipeline encapsulates preprocessing + estimator, simplifying deployment in Streamlit.\n- Next, leverage `models/model_performance.csv` for dashboarding and keep iterating on feature ideas if higher accuracy is required."
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}