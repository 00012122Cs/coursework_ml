{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c58fad763707d247",
   "metadata": {},
   "source": [
    "# 02 - Data Preprocessing\n",
    "This notebook focuses on preparing the life expectancy dataset for modelling by handling quality issues, engineering informative features, and exporting train/test splits."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a14546ac897584e9",
   "metadata": {},
   "source": [
    "## Pipeline Overview\n",
    "1. Load and standardise the raw WHO export.\n",
    "2. Remove duplicates and impossible values.\n",
    "3. Impute missing data using KNN for numeric columns and the mode for categoricals.\n",
    "4. Engineer temporal and regional features.\n",
    "5. Apply OneHotEncoder + scaling inside a `ColumnTransformer`.\n",
    "6. Perform an 80/20 train-test split and export processed artefacts to `data/processed/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f155173ce250297",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import json\n",
    "import re\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import KNNImputer, SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "\n",
    "PROJECT_ROOT = Path('..').resolve()\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "from streamlit_app import utils as app_utils\n",
    "\n",
    "CATEGORICAL_FEATURES = app_utils.CATEGORICAL_FEATURES\n",
    "FEATURE_COLUMNS = app_utils.FEATURE_COLUMNS\n",
    "NUMERIC_FEATURES = app_utils.NUMERIC_FEATURES\n",
    "TARGET_COLUMN = app_utils.TARGET_COLUMN\n",
    "\n",
    "DATA_PATH = PROJECT_ROOT / 'data' / 'life_expectancy.csv'\n",
    "PROCESSED_DIR = PROJECT_ROOT / 'data' / 'processed'\n",
    "PROCESSED_DIR.mkdir(parents=True, exist_ok=True)\n",
    "MODELS_DIR = PROJECT_ROOT / 'models'\n",
    "MODELS_DIR.mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4afee61ffdb8cbb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_life_expectancy_data(data_path: Path = DATA_PATH) -> pd.DataFrame:\n",
    "    text = data_path.read_text(encoding='utf-8').strip()\n",
    "    if text.startswith('{'):\n",
    "        payload = json.loads(text)\n",
    "        records = payload.get('value', [])\n",
    "        df = pd.DataFrame(records)\n",
    "    else:\n",
    "        df = pd.read_csv(data_path)\n",
    "    return df\n",
    "\n",
    "\n",
    "def to_snake_case(value: str) -> str:\n",
    "    value = value or ''\n",
    "    step1 = re.sub(r'(.)([A-Z][a-z]+)', r'\\1_\\2', value)\n",
    "    step2 = re.sub(r'([a-z0-9])([A-Z])', r'\\1_\\2', step1)\n",
    "    cleaned = re.sub(r'[^0-9a-zA-Z]+', '_', step2)\n",
    "    return '_'.join(filter(None, cleaned.lower().split('_')))\n",
    "\n",
    "\n",
    "def clean_column_names(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    df.columns = [to_snake_case(col) for col in df.columns]\n",
    "    return df\n",
    "\n",
    "\n",
    "def map_gender(value: str) -> str:\n",
    "    mapping = {\n",
    "        'sex_mle': 'Male',\n",
    "        'sex_fmle': 'Female',\n",
    "        'sex_btsx': 'Both sexes'\n",
    "    }\n",
    "    if not isinstance(value, str):\n",
    "        return 'Both sexes'\n",
    "    return mapping.get(value.lower(), value)\n",
    "\n",
    "\n",
    "def enrich_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    df['gender'] = df.get('dim1', 'SEX_BTSX').apply(map_gender)\n",
    "    df['country_code'] = df.get('spatial_dim')\n",
    "    df['continent_code'] = df.get('parent_location_code')\n",
    "    df['continent'] = df.get('parent_location')\n",
    "    df['year'] = df.get('time_dim').astype(int)\n",
    "    df['life_expectancy'] = df.get('numeric_value')\n",
    "    df['life_expectancy_low'] = df.get('low')\n",
    "    df['life_expectancy_high'] = df.get('high')\n",
    "    df['record_date'] = pd.to_datetime(df.get('date'), errors='coerce')\n",
    "    df['period_start'] = pd.to_datetime(df.get('time_dimension_begin'), errors='coerce')\n",
    "    df['period_end'] = pd.to_datetime(df.get('time_dimension_end'), errors='coerce')\n",
    "    df['value_range'] = df['life_expectancy_high'] - df['life_expectancy_low']\n",
    "    drop_cols = [\n",
    "        '@odata_context', 'dim1', 'dim1_type', 'dim2', 'dim2_type', 'dim3', 'dim3_type',\n",
    "        'time_dimension_value', 'value'\n",
    "    ]\n",
    "    df.drop(columns=[c for c in drop_cols if c in df.columns], inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc6ca1d5988d3555",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df = load_life_expectancy_data()\n",
    "df = enrich_columns(clean_column_names(raw_df))\n",
    "print(f'Initial shape: {df.shape}')\n",
    "\n",
    "valid_mask = df[TARGET_COLUMN].between(0, 120)\n",
    "df = df[valid_mask]\n",
    "df = df.drop_duplicates(subset=['country_code', 'year', 'gender'])\n",
    "print(f'Shape after quality filters: {df.shape}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281b3dc3cb039514",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['continent_encoded'] = df['continent'].astype('category').cat.codes\n",
    "year_min = df['year'].min()\n",
    "year_max = df['year'].max()\n",
    "df['year_normalized'] = (df['year'] - year_min) / (year_max - year_min)\n",
    "df['continent_life_expectancy_mean'] = df.groupby('continent')[TARGET_COLUMN].transform('mean')\n",
    "df['country_life_expectancy_mean'] = df.groupby('country_code')[TARGET_COLUMN].transform('mean')\n",
    "\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad6c356606a7100",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols = list(FEATURE_COLUMNS)\n",
    "target_col = TARGET_COLUMN\n",
    "\n",
    "X = df[feature_cols].copy()\n",
    "y = df[target_col].copy()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, shuffle=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c12ef71e881176b",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_features = list(NUMERIC_FEATURES)\n",
    "categorical_features = list(CATEGORICAL_FEATURES)\n",
    "\n",
    "numeric_transformer = Pipeline(\n",
    "    steps=[\n",
    "        ('imputer', KNNImputer(n_neighbors=5)),\n",
    "        ('scaler', StandardScaler())\n",
    "    ]\n",
    ")\n",
    "\n",
    "categorical_transformer = Pipeline(\n",
    "    steps=[\n",
    "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "        ('encoder', OneHotEncoder(handle_unknown='ignore', sparse=False))\n",
    "    ]\n",
    ")\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('numeric', numeric_transformer, numeric_features),\n",
    "        ('categorical', categorical_transformer, categorical_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "preprocessor.fit(X_train)\n",
    "X_train_processed = preprocessor.transform(X_train)\n",
    "X_test_processed = preprocessor.transform(X_test)\n",
    "\n",
    "feature_names = preprocessor.get_feature_names_out()\n",
    "X_train_processed_df = pd.DataFrame(X_train_processed, columns=feature_names)\n",
    "X_test_processed_df = pd.DataFrame(X_test_processed, columns=feature_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c33c41c0092ec05",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_path = PROCESSED_DIR / 'life_expectancy_clean.csv'\n",
    "train_path = PROCESSED_DIR / 'X_train_processed.csv'\n",
    "test_path = PROCESSED_DIR / 'X_test_processed.csv'\n",
    "y_train_path = PROCESSED_DIR / 'y_train.csv'\n",
    "y_test_path = PROCESSED_DIR / 'y_test.csv'\n",
    "\n",
    "df.to_csv(clean_path, index=False)\n",
    "X_train_processed_df.to_csv(train_path, index=False)\n",
    "X_test_processed_df.to_csv(test_path, index=False)\n",
    "y_train.to_csv(y_train_path, index=False, header=True)\n",
    "y_test.to_csv(y_test_path, index=False, header=True)\n",
    "\n",
    "joblib.dump(preprocessor, MODELS_DIR / 'preprocessor.pkl')\n",
    "print('Exported cleaned dataset, processed splits, and fitted preprocessor.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66cbc4552579d47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({\n",
    "    'dataset': ['X_train_processed', 'X_test_processed', 'y_train', 'y_test'],\n",
    "    'rows': [len(X_train_processed_df), len(X_test_processed_df), len(y_train), len(y_test)],\n",
    "    'columns': [X_train_processed_df.shape[1], X_test_processed_df.shape[1], 1, 1]\n",
    "})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
